---
layout: post
title: Notes on SVM 
date:   2016-08-05 10:10:00
tags:  MachineLearning 
subclass: 'post tag-MachineLearning'
categories: 'casper'
cover: 'assets/images/valley_with_ball.png'
navigation: True
logo: 'assets/images/ghost.png'
---
$~$

**博主注：以下纯属自己的理解，不合理的地方，非常期待读者能给予指正; 如果读者对某部分能提出更好的解释，那就更好了。**

#### 引入

对于一个新的样例（example): $~x = (x_0, x_1, ..., x_n)^T~$，其中$~x_0 = 1~$, 
在 $~logistic~$ 回归中，首先将这个样例的各个特征 (feature) 以权值
$~\theta = (\theta_0, \theta_1, ...\theta_n)^T~$
加权相加，得到的结果$\theta^T x$通过一个 $~sigmoid~$ 函数，
映射成一个 $(0,1)$ 区间内的值。然后以 $~0.5~$ 为临界点，
当大于 $~0.5~$ 时，我们把它判定为一类（比如正类），否则我们把它判定成另一类（比如负类)。
而 $~sigmoid~$ 的输出越接近 $~1~$ 或 $~0~$ 我们对这个分类的结果就越有信心。

$~sigmoid~$ 函数是一个单调递增函数，当输入为$~0~$时，输出恰好是$~0.5~$。 
因此实际上可以通过对$~\theta^T x~$进行判断，如果它大于$~0~$，那么把它预测为正类，
否则为负类。它的绝对值越大，那么我们对预测结果的信心就越强。

SVM的第一个想法就是将这个“信心”量化。

#### Margin

在上面的讨论中，可以把$~\theta^T x = 0~$看成是将样例分隔开来的一个超平面。一个样例$~x~$里超平面越远，对应$~\|\theta^T x\|~$就越大，那么分类的结果就越准确。
为了引出函数间隔(functional margin)以及几何间隔(geometric margin)的定义，
将$~\theta^T x~$改写成$~w^Tx+b~$, 其中 $~w = (\theta_1, ..., \theta)^T, ~
x = (x_1, ..., x_n)^T, ~b = \theta_0~$.

对于二分类问题中的一个训练样例$~x^{(i)}~$，记其标签$~y^{(i)}\in\\{-1, 1\\}$, 那么 $y^{(i)}(w^T x^{(i)} + b) > 0$时就表示分类正确，且其越大，分类结果可信度就越高。由此引出函数间隔(functinal margin)的定义:

$$\hat{\gamma}^{(i)} = y^{(i)}(w^T x^{(i)} + b)$$

用函数间隔来衡量可信度有一个问题，就是没有明确的量度标准，比如说你可以认为它的单位是厘米，也可以认为是米，甚至可以认为是千米或者纳米。也就是我们可以对函数结果进行随意的缩放，都不影响分类的结果。因此函数间隔并不可靠。我们需要一个更明确的量度标准。

如果把这个量度标准也就是单位定为 $\|\|w\|\|$， 那么得到了几何间隔(geometric margin):

$$\gamma^{(i)} = y^{(i)}(\frac{w^T}{||w||} x^{(i)} + \frac{b}{||w||})$$

由此可见，几何间隔就是以 $\|\|w\|\|$单位化后的函数间隔，即有$\hat{\gamma}^{(i)} = \gamma ^{(i)} * \|\|w\|\|$.

定义最小函数间隔$\hat{\gamma}$ 和最小几何间隔 $\gamma$:

$$\hat{\gamma}= arg~\mathop{min}\limits_i\hat{\gamma}^{(i)}$$

$$\gamma= arg~\mathop{min}\limits_i\gamma^{(i)}$$

#### 最优间隔分类器

**(注：此部分需要进一步斟酌)**

给定训练数据集$S = \\{(x^{(i)}, y^{(i)}); i = 1, ..., m\\}$, **假设数据集线性可分**，那么一个自然的想法是找到一个超平面作为分界面，使得这些样例到分类平面的最小几何间隔最大。用数学语言来描述，可以形成以下优化问题：

$$
\begin{align}
max_{\gamma, w, b} \ ~~~ & \gamma\\\
s.t. \  ~~~              &\frac{ y^{(i)}(w^T x^{(i)} + b)}{||w||} \ge \gamma, ~i = 1, ..., m    
\end{align}
$$

意思是说，最大化“最小几何间隔”$\gamma$，使得所有的几何间隔都至少为“最小几何间隔”$\gamma$.

由于现有的优化工具包只能解决凸优化问题(Convex Optimization Problem)，而上面的约束并不是凸集，因而没办法解，我们需要把它变成凸优化问题。上面说到几何间隔就是以$\|\|w\|\|$单位化的函数间隔，但是$\|\|w\|\|$应该怎么选取，并没有说到。事实上，$\|\|w\|\|$可以取任何值，犹如函数间隔的单位可以明确为任何单位(**这种观点有进一步待解释和考证**)。因此我们可以令$\|\|w\|\| = \frac{1}{\gamma}$, 变为以下形式：

$$
\begin{align}
max_{\gamma, w, b} \ ~~~ & \frac{1}{||w||}\\\
s.t. \  ~~~              & y^{(i)}(w^T x^{(i)} + b) \ge 1, ~i = 1, ..., m\\\
\end{align}
$$

或者

$$
\begin{align}
min_{\gamma, w, b} \ ~~~ & \frac{1}{2}||w||^2 \\\
s.t. \  ~~~              & y^{(i)}(w^T x^{(i)} + b) \ge 1, ~i = 1, ..., m \\\
\end{align}
$$

至此，我们已经将原来的非凸优化问题转化成了凸优化问题，终于祭出现有的凸优化工具包地愉快地进行求解了。

#### 拉格朗日对偶形式
