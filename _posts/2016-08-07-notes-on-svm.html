---
layout: post
title: Notes on SVM 
date:   2016-08-05 10:10:00
tags:  MachineLearning 
subclass: 'post tag-MachineLearning'
categories: 'casper'
cover: 'assets/images/valley_with_ball.png'
navigation: True
logo: 'assets/images/ghost.png'
---
<p></p>

<hr/>
<h3 id="heading3"> 引入 </h3>
<hr />

<p>  
对于一个新的样例（example)， 在 $logistic$ 回归中，首先将这个样例的各个特征 (feature) 加权相加 ($x_0 = 1$ 看作是它的一个特征)，得到的结果通过一个 $sigmoid$ 函数，映射成一个 $0~1$ 的值。然后以 $0.5$ 为临界点，当大于 $0.5$ 时，我们把它判定为一类（比如正类），否则我们把它判定成另一类（比如负类）。而 $sigmoid$ 的输出越接近 $1$ 或 $0$ 我们对这个分类的结果就越有信心。

</p>

<p>
$sigmoid$ 函数是一个单调递增函数，当输入为$0$时，输出恰好是$0.5$。 因此实际上可以通过对加权相加的值进行判断，如果它大于$0$，那么把它预测为正类，否则为负类。它的绝对值越大，那么我们对预测结果的信心就越强。
</p>

<p>
SVM的第一个想法就是将这个信心量化。
</p>



