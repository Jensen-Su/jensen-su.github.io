<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>jensen-su.github.io/jasper/</title>
   
   <link>http://jensen-su.github.io/jasper/</link>
   <description>Doubt thout the stars are fire, Doubt that the sun doth move, Doubt truth to be a liar, But never doubt the lights</description>
   <language>en-uk</language>
   <managingEditor> Jensen Su</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Notes on SVM</title>
	  <link>/jasper//notes-on-svm</link>
	  <author>Jensen Su</author>
	  <pubDate>2016-08-05T18:10:00+08:00</pubDate>
	  <guid>/jasper//notes-on-svm</guid>
	  <description><![CDATA[
	     <p></p>

<hr/>
<h3 id="heading3"> 引入 </h3>
<hr />

<p>  
对于一个新的样例（example)， 在 $logistic$ 回归中，首先将这个样例的各个特征 (feature) 加权相加 ($x_0 = 1$ 看作是它的一个特征)，得到的结果通过一个 $sigmoid$ 函数，映射成一个 $0~1$ 的值。然后以 $0.5$ 为临界点，当大于 $0.5$ 时，我们把它判定为一类（比如正类），否则我们把它判定成另一类（比如负类）。而 $sigmoid$ 的输出越接近 $1$ 或 $0$ 我们对这个分类的结果就越有信心。

</p>

<p>
$sigmoid$ 函数是一个单调递增函数，当输入为$0$时，输出恰好是$0.5$。 因此实际上可以通过对加权相加的值进行判断，如果它大于$0$，那么把它预测为正类，否则为负类。它的绝对值越大，那么我们对预测结果的信心就越强。
</p>

<p>
SVM的第一个想法就是将这个信心量化。
</p>




	  ]]></description>
	</item>

	<item>
	  <title>The Learning Theroy</title>
	  <link>/jasper//learning-theory</link>
	  <author>Jensen Su</author>
	  <pubDate>2016-07-30T18:10:00+08:00</pubDate>
	  <guid>/jasper//learning-theory</guid>
	  <description><![CDATA[
	     <p>$~$</p>

<h4 id="openning-words">1. Openning words</h4>

<p>我们知道，在机器学习中，当模型过于简单(数据集很大)时，往往会发生<strong>欠拟合(underfitting)</strong>，也就是模型的学习能力太弱，没有很好地捕获到数据集的共性，当用该模型对新数据进行预测时，就会发生偏差;当模型过于复杂(数据集却很小)时，往往很容易发生<strong>过拟合(overfitting)</strong>，也就是模型学习能力太强，把数据中的扰动误以为是数据集的共同特性，当用该模型进行预测时，就会对扰动(比如噪声)十分敏感。</p>

<p>对此周志华教授的《机器学习》有个很好的例子，比如我们采集了一些树叶作为训练集，尝试对树叶进行学习，使得习得的模型可以预测新样例是不是树叶。如果训练集很大，包含了各种奇形怪状的样子，一个欠拟合的模型可能会把所有绿色的新样例都判定为树叶;如果训练集很小，比如恰好采集的树叶的边缘都具有锯齿，一个过拟合的模型将认为锯齿是所有树叶的共性，并将没有锯齿的树叶认定为不是树叶。</p>

<p>这是欠拟合和过拟合的直观理解。不管发生欠拟合还是过拟合，该模型用于预测新的样例时，都会发生明显的误差，这个误差就是<strong>泛化误差(generalization error)</strong>。实际上，泛化误差可以分为三个部分的叠加。也就是偏差(bias)、方差(variance)、和噪声(noise)。当发生欠拟合，偏差比重上升占据主导地位，当发生过拟合时，方差比重则上升并占据主导地位。</p>

<p>于是引出了第一个问题：</p>

<p><strong>问题1</strong> 偏差和方差看起来是此消彼长的关系，我们应该对它们进行折衷，那么我们能否对它们进行量化，从而从理论上找到一个最好的折衷点呢？</p>

<p>从上面的讨论中发现，我们最关心的其实是模型对新样例的预测能力，或者说泛化误差，然而我们的模型却是在训练集中习得，所习得的模型对于训练集中的样例的预测误差称为经验误差。那么就有了下面的问题：</p>

<p><strong>问题2</strong> 在训练集上表现良好的模型面对新样例时表现如何呢？训练误差和泛化误差之间是否存在可以量化的联系？</p>

<p><strong>问题3</strong> 如果这种可量化的关系的存在，是否可以证明？需要什么假设呢？</p>

<h4 id="premininaries">2. Premininaries</h4>

<p>在对这些问题进行探讨之前，我们先引出学习理论中许多理论所依赖的PAC(probably approximately corrent, 可能大概是对的)框架中极为重要的一条假设：</p>

<p><strong>训练集和测试集中的样本相互独立同分布</strong></p>

<p>本文假设这一分布为<strong>伯努利分布</strong>，也就是<strong>二分类</strong>问题，得到的结果同样适用于其他分布。</p>

<h6 id="notations">2.1 Notations</h6>

<ul>
  <li>分布(伯努利):  $\mathcal{D}$</li>
  <li>训练集：       $S = {(x^{(i)}, y^{(i)}); i = 1, …, m}$</li>
  <li>训练集大小：   $m$</li>
  <li>特征向量:      $x^{(i)}$</li>
  <li>对应标签：     $y^{(i)}$</li>
  <li>特征向量集：   $x$</li>
  <li>对应标签集：   $y$</li>
  <li>假设:          $h$ 或 $h_\theta$,其中$\theta$为假设中待训练的模型参数</li>
  <li>训练误差：     $\hat{\epsilon}(h)$</li>
  <li>泛化误差：     $\epsilon (h)$</li>
  <li>假设空间：     $\mathcal{H}$</li>
  <li>假设空间大小： $k$</li>
  <li>置信度:        $1-\delta,~\delta &gt; 0$</li>
</ul>

<p><strong>定义1. 训练误差</strong> $\hat{\epsilon}(h) = \frac{1}{m}\sum_{i=1}^{m} 1{h(x^{(i)}) \neq y^{(i)}}.$ 也就是 训练集中 被假设 $h$ 误分类的样本比例，也称为<em>经验风险</em>或者*经验误差。</p>

<p><strong>定义2. 泛化误差</strong> $\epsilon (h) = P_{(x,y)\sim\mathcal{D}}(h(x) \neq y).$ 也就是当我们从分布$\mathcal{D}$中抽取一个新样例，假设$h$对其误分类的概率。</p>

<h6 id="assumptions">2.2 Assumptions</h6>

<p><strong>假设1</strong> 对于给定的假设，我们是通过<strong>经验最小化(EMR)</strong>的方法来选取模型参数：</p>

<script type="math/tex; mode=display">\hat{\theta} = arg~\mathop{min}\limits_\theta\hat{\epsilon}(h_\theta)</script>

<p>意思就是选取使得训练误差最小的模型参数。</p>

<p><strong>假设2</strong> 对于给定的假设空间 $\mathcal{H}$, 我们也通过经验最小化来选出最优假设：</p>

<script type="math/tex; mode=display">\hat{h} = arg~\mathop{min}\limits_{h \in \mathcal{H}}\hat{\epsilon}(h_\theta)</script>

<p>意思就是在假设空间里面选取使得训练误差最小的假设。</p>

<h4 id="conclusion">3. Conclusion</h4>
<p>做完以上的准备工作之后，我们直接给出结论。对于结论的证明以后再补上。</p>

<p>假设我们有一个大小为$k$的假设集 $\mathcal{H} = {h_1, …, h_k}$. 给定训练样本数 $m$, 给定 $\gamma &gt; 0$, 
那么所有的假设$h_i$的泛化误差$\epsilon (h)$ 偏离经验误差 $\hat{\epsilon}(h)$ 均不大于 $\gamma$ 的概率至少为</p>

<script type="math/tex; mode=display">1 - 2k~exp(-2\gamma^2m)</script>

<p>即<strong>一致收敛</strong>(<em>uniform convergence</em>)结论. （也可以这样看，至少有一个假设的经验误差偏离泛化误差$\gamma$之外的概率不大于$2k~exp(-2\gamma^2m)$.)</p>

<p><strong>问题1</strong>： 给定$\gamma$和置信度$1 - \delta$, $m$ 必须取多大才能保证对于训练误差与泛化误差的偏差$|\epsilon(h)-\hat{\epsilon}(h)| \le \gamma$？</p>

<p><strong>结论</strong>: 令$1 - 2k~exp(-2\gamma^2m) \ge 1 - \delta$, 可以求得$m$的一个下界：</p>

<script type="math/tex; mode=display">m \ge \frac{1}{2\gamma^2}log\frac{2k}{\delta}.</script>

<p>这告诉我们：</p>

<ol>
  <li>样本数与给定偏差 $\gamma$ 成二次反比，因此样本越大，训练误差与泛化误差相差越小.</li>
  <li><strong>更有指导意义的是，训练集大小$m$的下界与假设空间$\mathcal{H}$的大小只是对数关系.</strong></li>
</ol>

<p>为了保证一定的性能，一个算法所要求训练集的大小 $m$ 称为该算法的<strong>取样复杂度(sample complexity)</strong>.</p>

<p><strong>问题2</strong>： 给定训练集大小$m$和置信度$1-\delta&gt;0$, 训练误差与泛化误差的偏差$|\epsilon(h)-\hat{\epsilon}(h)|$ 的上界是多少？</p>

<p><strong>结论1</strong>: 令$2k~exp(-2\gamma^2m) = \delta$, 可以求得$\gamma$的一个上界：</p>

<script type="math/tex; mode=display">|\epsilon(h)-\hat{\epsilon}(h)| \le \sqrt{\frac{1}{2m}log\frac{2k}{\delta}}.</script>

<p><strong>注意，这是对于假设空间的所有假设$h\in\mathcal{H}$都成立的</strong>. 那么对于我们根据经验最小化原则选取的假设$\hat{h} = arg~\mathop{min}\limits_{h \in \mathcal{H}}\hat{\epsilon}(h_\theta)$来说会有什么样的上界呢？</p>

<p>我们把问题2的结论中$\gamma$的上界记为$\gamma_0$. 即$ |\epsilon(h)-\hat{\epsilon}(h)| \le \gamma_0$.</p>

<p>对于我们选取的假设$\hat{h}$有$|\epsilon(\hat{h})-\hat{\epsilon}(\hat{h})|\le\gamma_0$, 或者 $\epsilon(\hat{h}) \le \hat{\epsilon}(\hat{h}) + \gamma_0$.</p>

<p>对于最优假设$h^*$, 同样有$|\hat{\epsilon}(h^*)-\epsilon(h^*)|\le\gamma_0$, 或者 $\hat{\epsilon}(h^*) \le \epsilon(h^*) + \gamma_0$.</p>

<p><strong>结论2</strong>因此对于我们选取的假设有：</p>

<script type="math/tex; mode=display">\epsilon(\hat{h}) \le \hat{\epsilon}(\hat{h}) + \gamma_0 
                    \le \hat{\epsilon}(h^*) + \gamma_0 
                    \le \epsilon(h^*) + \gamma_0</script>

<p>也就是说在一致收敛的前提下，根据经验最小化原则习得的假设的泛化误差比假设空间里面最好的假设的泛化误差相差不大于$2\gamma_0$!</p>

<p><strong>定理</strong> 设 $|\mathcal{H}|=k$, 给定训练集大小$m$和置信度$1-\delta$, 对于由经验最小化习得的假设，有：</p>

<script type="math/tex; mode=display">\epsilon(\hat{h}) \le \left(\mathop{min}\limits_{h\in\mathcal{H}}\epsilon(h)\right)+ 2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}</script>

<p>这个定理量化了我们讨论的偏差与方差间的折衷问题。例如原来的假设空间为$\mathcal{H}$,如果我们考虑一个更大的假设空间 $\mathcal{H’} \supsetneqq \mathcal{H}$, 那么上式第一项会减小，也就是偏差减小;然而由于假设空间变大，即$k$变大，会使得第二项变大，也就是方差变大。</p>

	  ]]></description>
	</item>

	<item>
	  <title>The Gradient Descent</title>
	  <link>/jasper//gradient-descent</link>
	  <author>Jensen Su</author>
	  <pubDate>2016-07-11T18:10:00+08:00</pubDate>
	  <guid>/jasper//gradient-descent</guid>
	  <description><![CDATA[
	     <p> </p>
<p> The cover picture is taken from Michael Nielsen's book 
<a href="http://neuralnetworksanddeeplearning.com/index.html"><em> Neural Networks and Deep Learning</em>.
<p><a href="https://en.wikipedia.org/wiki/Gradient_descent"><strong>Gradient descent</strong></a>, 
also known as <em>steepest descent</em>, is a first-order optimization algorithm mostly used to 
find a local minimum of a given contunious, differentiable function. For convex functions,
the local minimum is also the global minimum. Therefore gradient descent is widely used 
to solve the unconstrained optimization problem. One of its versions is also the most commonly 
used algorithm to train the neural networks in deep learning.
</p>

<p> Gradient descent is a powerful but very simple algorithm. Given a function $C(\vec x)$ having 
first order partial derivatives (w.r.t $\vec x$), assuming the function has global minimum, then
gradient descent finds a global minimum with the following iterations:

$$ \vec x_0 := [random ~initialization]\\
\vec x_{t+1} := \vec x_t -  \eta\nabla_C(\vec x_t)$$

where $\eta$ controls the step size of each iteration. In some well-designed versions of gradient 
descent, $\eta$ may vary at each iteration.
</p>

<p> It is quite simple and clear enough for anybody who can work out the gradient to realize 
such algorithm. However simple and straightforward, I find myself unconvinced. 
How comes it with this form? How does it make sense? 
Why does $\vec x$ minus (rather than plus) the term $\eta\nabla_C(\vec x_t)$?
</p>

<p> After a few derivations, I gained some insights into it.</p>

<p> Recall from calculus that, if we were to make some small changes $\Delta x$ on $\vec x$, the changes on
output $C$ would be 
$$\Delta C = \nabla_C \cdot \Delta x.$$
How should we choose the $\Delta x$, such that $\Delta C$ always be negative, knowing we would like the objective
function $C(\vec x)$ to decrease a small amount each iteration to reach a minimum? The eaiest way would
be to choose $\Delta x = -\nabla_C$, such that $\Delta C = - \nabla_C^2$ would be negative.
</p>

<p> Now we've known how it comes with the form $\vec x_{t+1} := \vec x_t -  \eta\nabla_C(\vec x_t)$. It decreases 
$\vec x$ by a small amount of $\eta\nabla_C(\vec x_t)$ each iteration to decrease the objective function $C$ 
by a small amount, hopefully to reach a minimum after some iterations.
</p>

<p> It is therefore makes sense that we could take the form  $\vec x_{t+1} := \vec x_t + \eta\nabla_C(\vec x_t)$ to
find a maximum of a objective function.
</p>

<p> But I still not be completely convinced. Is the choice of $\Delta x$ the fastest way to decrease the 
objective function? Put it another way, given a certain $\eta \lt 0$, is there any other better choice of
$\Delta x$? 
</p>
<p>
Recall that $\Delta C = \nabla_C \cdot \Delta x$, the question is to find a $\Delta x$ to minimize
$\Delta C$. Since $\nabla_C$ and $\Delta x$ are both vectors, we can write $\Delta C$ as
$\Delta C = ||\nabla_C|| \cdot ||\Delta x|| \cdot sin\theta$. It tells the choice of $\Delta x$ which 
minimize $\nabla_C \cdot \Delta x$ is $ -\eta \Delta x$.
</p>

<p>
Although gradient descent is the steepest descent, it is not neccessary the one that convergences fartest. 
Instead, it sometimes convergences much slow, wandering about around the nearby minimum.
Some other algorithms that convergence faster like <em>Newton method </em> and <em> quasi Newton method </em>
are two-order optimization algorithms. I plan to investigate and summarize these two methods in the next post. 
</p>

<p> Since It is mostly my self-understanding, there probably be some mistakes.
Please kindly leave your comments, thank you^.^ </p>

	  ]]></description>
	</item>

	<item>
	  <title>My First Post</title>
	  <link>/jasper//my-first-post</link>
	  <author>Jensen Su</author>
	  <pubDate>2016-07-10T18:10:00+08:00</pubDate>
	  <guid>/jasper//my-first-post</guid>
	  <description><![CDATA[
	     <p> Hello world, this is my first post. </p>

<p> The Jekyll theme is the <a href="https://github.com/biomadeira/jasper">Jasper Theme</a>. </p>

<p> $$\frac{x^2}{\sqrt{y+1}}$$ </p>

<p> Thanks. </p>

	  ]]></description>
	</item>


</channel>
</rss>
