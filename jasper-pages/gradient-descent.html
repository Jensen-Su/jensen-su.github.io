<!DOCTYPE html>
<html>
<head>
    <!--mathjax-->
    <script type="text/javascript" 
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}}); </script>

    <style>
    /* Adapted from */
    /* https://groups.google.com/d/msg/mathjax-users/jqQxrmeG48o/oAaivLgLN90J, */
    /* by David Cervone */

    @font-face {
        font-family: 'MJX_Math';
        src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); /* IE9 Compat Modes */
        src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot?iefix') format('eot'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff')  format('woff'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf')  format('opentype'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/svg/MathJax_Math-Italic.svg#MathJax_Math-Italic') format('svg');
    }

    @font-face {
        font-family: 'MJX_Main';
        src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); /* IE9 Compat Modes */
        src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot?iefix') format('eot'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff')  format('woff'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf')  format('opentype'),
        url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/svg/MathJax_Main-Regular.svg#MathJax_Main-Regular') format('svg');
    }
    </style>
    <!-- [[! Document Settings ]] -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- [[! Page Meta ]] -->
    <title>The Gradient Descent</title>
    <meta name="description" content="Firefly In The Darkness, Flittering About - Doubt thout the stars are fire, Doubt that the sun doth move, Doubt truth to be a liar, But never doubt the lights" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/jasper/assets/images/favicon.ico" >

    <!-- [[! Styles'n'Scripts ]] -->
    <link rel="stylesheet" type="text/css" href="/jasper/assets/css/screen.css" />
    <link rel="stylesheet" type="text/css"
          href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
    <link rel="stylesheet" type="text/css" href="/jasper/assets/css/syntax.css" />

    <!-- [[! highlight.js ]] -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <!-- [[! Ghost outputs important style and meta data with this tag ]] -->
        <link rel="canonical" href="/jasper/" />
    <meta name="referrer" content="origin" />
    <link rel="next" href="/jasper/page2/" />

    <meta property="og:site_name" content="Firefly In The Darkness, Flittering About" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Firefly In The Darkness, Flittering About" />
    <meta property="og:description" content="Doubt thout the stars are fire, Doubt that the sun doth move, Doubt truth to be a liar, But never doubt the lights" />
    <meta property="og:url" content="/jasper/" />
    <meta property="og:image" content="/jasper/assets/images/cover1.jpg" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Firefly In The Darkness, Flittering About" />
    <meta name="twitter:description" content="Doubt thout the stars are fire, Doubt that the sun doth move, Doubt truth to be a liar, But never doubt the lights" />
    <meta name="twitter:url" content="/jasper/" />
    <meta name="twitter:image:src" content="/jasper/assets/images/cover1.jpg" />

    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Website",
    "publisher": "Finding The Way Home",
    "url": "/jasper/",
    "image": "/jasper/assets/images/cover1.jpg",
    "description": "Doubt thout the stars are fire, Doubt that the sun doth move, Doubt truth to be a liar, But never doubt the lights"
}
    </script>

    <meta name="generator" content="Jekyll 3.0.0" />
    <link rel="alternate" type="application/rss+xml" title="Firefly In The Darkness, Flittering About" href="/jasper/feed.xml" />


</head>

<body class="home-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        <li class="nav-home " role="presentation"><a href="/jasper/">Home</a></li>
        <li class="nav-about " role="presentation"><a href="/jasper/about">About</a></li>
        <li class="nav-fables " role="presentation"><a href="/jasper/tag/fables">Fables</a></li>
        <li class="nav-speeches " role="presentation"><a href="/jasper/tag/speeches">Speeches</a></li>
        <li class="nav-fiction " role="presentation"><a href="/jasper/tag/fiction">Fiction</a></li>
        <li class="nav-author " role="presentation"><a href="/jasper/author/casper">Author</a></li>
    </ul>
    <a class="subscribe-button icon-feed" href="/jasper/feed.xml">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        <!-- [[! Everything else gets inserted here ]] -->
        <!-- < default -->

<!-- The comment above "< default" means - insert everything in this file into -->
    <!-- the [body] of the default.hbs template, which contains our header/footer. -->

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<header class="main-header post-head " style="background-image: url(/jasper/assets/images/valley_with_ball.png) ">
    <nav class="main-nav  overlay  clearfix">
        <a class="blog-logo" href="/jasper/"><img src="/jasper/assets/images/ghost.png" alt="Blog Logo" /></a>
        
            <a class="menu-button icon-menu" href="#"><span class="word">Menu</span></a>
        
    </nav>
</header>

<main class="content" role="main">

    <article class="post tag-NeuralNetwork">

        <header class="post-header">
            <h1 class="post-title">The Gradient Descent</h1>
            <section class="post-meta">
            <!-- <a href='/jasper/'>Jensen Su</a> -->
            <time class="post-date" datetime="2016-07-11">11 Jul 2016</time>
                <!-- [[tags prefix=" on "]] -->
                 
                on 
                
                    
                       <a href='/jasper/tag/NeuralNetwork'>Neuralnetwork</a>
                       
                
                
            </section>
        </header>

        <section class="post-content">
            
            <p> </p>
<p> The cover picture is taken from Michael Nielsen's book 
<a href="http://neuralnetworksanddeeplearning.com/index.html"><em> Neural Networks and Deep Learning</em>.
<p><a href="https://en.wikipedia.org/wiki/Gradient_descent"><strong>Gradient descent</strong></a>, 
also known as <em>steepest descent</em>, is a first-order optimization algorithm mostly used to 
find a local minimum of a given contunious, differentiable function. For convex functions,
the local minimum is also the global minimum. Therefore gradient descent is widely used 
to solve the unconstrained optimization problem. One of its versions is also the most commonly 
used algorithm to train the neural networks in deep learning.
</p>

<p> Gradient descent is a powerful but very simple algorithm. Given a function $C(\vec x)$ having 
first order partial derivatives (w.r.t $\vec x$), assuming the function has global minimum, then
gradient descent finds a global minimum with the following iterations:

$$ \vec x_0 := [random ~initialization]\\
\vec x_{t+1} := \vec x_t -  \eta\nabla_C(\vec x_t)$$

where $\eta$ controls the step size of each iteration. In some well-designed versions of gradient 
descent, $\eta$ may vary at each iteration.
</p>

<p> It is quite simple and clear enough for anybody who can work out the gradient to realize 
such algorithm. However simple and straightforward, I find myself unconvinced. 
How comes it with this form? How does it make sense? 
Why does $\vec x$ minus (rather than plus) the term $\eta\nabla_C(\vec x_t)$?
</p>

<p> After a few derivations, I gained some insights into it.</p>

<p> Recall from calculus that, if we were to make some small changes $\Delta x$ on $\vec x$, the changes on
output $C$ would be 
$$\Delta C = \nabla_C \cdot \Delta x.$$
How should we choose the $\Delta x$, such that $\Delta C$ always be negative, knowing we would like the objective
function $C(\vec x)$ to decrease a small amount each iteration to reach a minimum? The eaiest way would
be to choose $\Delta x = -\nabla_C$, such that $\Delta C = - \nabla_C^2$ would be negative.
</p>

<p> Now we've known how it comes with the form $\vec x_{t+1} := \vec x_t -  \eta\nabla_C(\vec x_t)$. It decreases 
$\vec x$ by a small amount of $\eta\nabla_C(\vec x_t)$ each iteration to decrease the objective function $C$ 
by a small amount, hopefully to reach a minimum after some iterations.
</p>

<p> It is therefore makes sense that we could take the form  $\vec x_{t+1} := \vec x_t + \eta\nabla_C(\vec x_t)$ to
find a maximum of a objective function.
</p>

<p> But I still not be completely convinced. Is the choice of $\Delta x$ the fastest way to decrease the 
objective function? Put it another way, given a certain $\eta \lt 0$, is there any other better choice of
$\Delta x$? 
</p>
<p>
Recall that $\Delta C = \nabla_C \cdot \Delta x$, the question is to find a $\Delta x$ to minimize
$\Delta C$. Since $\nabla_C$ and $\Delta x$ are both vectors, we can write $\Delta C$ as
$\Delta C = ||\nabla_C|| \cdot ||\Delta x|| \cdot sin\theta$. It tells the choice of $\Delta x$ which 
minimize $\nabla_C \cdot \Delta x$ is $ -\eta \Delta x$.
</p>

<p>
Although gradient descent is the steepest descent, it is not neccessary the one that convergences fartest. 
Instead, it sometimes convergences much slow, wandering about around the nearby minimum.
Some other algorithms that convergence faster like <em>Newton method </em> and <em> quasi Newton method </em>
are two-order optimization algorithms. I plan to investigate and summarize these two methods in the next post. 
</p>

<p> Since It is mostly my self-understanding, there probably be some mistakes.
Please kindly leave your comments, thank you^.^ </p>


        </section>

        <footer class="post-footer">

            <!-- Everything inside the #author tags pulls data from the author -->
            <!-- #author-->

            
            <figure class="author-image">
                <a class="img" href="/jasper/author/shindou" style="background-image: url(/jasper/assets/images/casper.png)"><span class="hidden">'s Picture</span></a>
            </figure>
            

            <section class="author">
                <h4><a href="/jasper/author/shindou">Jensen Su</a></h4>
                
                
                    <p> State Key Lab. of Asic & Sys. Fudan University.</p>
                
                <div class="author-meta">
                    <span class="author-location icon-location"> Shanghai, CH</span> 
                    <span class="author-link icon-link"><a href="http://jensen-su.github.io/jasper/"> jensen-su.github.io/jasper/</a></span> 
                </div>
            </section>

            <!-- /author  -->

            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="http://twitter.com/share?text=The Gradient Descent&amp;url=http://jensen-su.github.io/jasper/gradient-descent"
                    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://jensen-su.github.io/jasper/gradient-descent"
                    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://jensen-su.github.io/jasper/gradient-descent"
                   onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>
            
            <!-- Add Disqus Comments -->
            
            
        </footer>

    </article>

</main>

<aside class="read-next">

    <!-- [[! next_post ]] -->
    
        <a class="read-next-story " style="background-image: url(/jasper/assets/images/valley_with_ball.png)" href="/jasper/learning-theory">
            <section class="post">
                <h2>The Learning Theroy</h2>
                <p>$~$ ####1. Openning words 我们知道，在机器学习中，当模型过于简单(数据集很大)时，往往会发生**欠拟合(underfitting)**，也就是模型的学习能力太弱，没有很好地捕获到数据集的共性，当用该模型对新数据进行预测时，就会发生偏差;当模型过于复杂(数据集却很小)时，往往很容易发生**过拟合(overfitting)**，也就是模型学习能力太强，把数据中的扰动误以为是数据集的共同特性，当用该模型进行预测时，就会对扰动(比如噪声)十分敏感。 对此周志华教授的《机器学习》有个很好的例子，比如我们采集了一些树叶作为训练集，尝试对树叶进行学习，使得习得的模型可以预测新样例是不是树叶。如果训练集很大，包含了各种奇形怪状的样子，一个欠拟合的模型可能会把所有绿色的新样例都判定为树叶;如果训练集很小，比如恰好采集的树叶的边缘都具有锯齿，一个过拟合的模型将认为锯齿是所有树叶的共性，并将没有锯齿的树叶认定为不是树叶。 这是欠拟合和过拟合的直观理解。不管发生欠拟合还是过拟合，该模型用于预测新的样例时，都会发生明显的误差，这个误差就是**泛化误差(generalization error)**。实际上，泛化误差可以分为三个部分的叠加。也就是偏差(bias)、方差(variance)、和噪声(noise)。当发生欠拟合，偏差比重上升占据主导地位，当发生过拟合时，方差比重则上升并占据主导地位。 于是引出了第一个问题： **问题1** 偏差和方差看起来是此消彼长的关系，我们应该对它们进行折衷，那么我们能否对它们进行量化，从而从理论上找到一个最好的折衷点呢？ 从上面的讨论中发现，我们最关心的其实是模型对新样例的预测能力，或者说泛化误差，然而我们的模型却是在训练集中习得，所习得的模型对于训练集中的样例的预测误差称为经验误差。那么就有了下面的问题： **问题2** 在训练集上表现良好的模型面对新样例时表现如何呢？训练误差和泛化误差之间是否存在可以量化的联系？ **问题3**...</p>
            </section>
        </a>
    
    <!-- [[! /next_post ]] -->
    <!-- [[! prev_post ]] -->
    
        <a class="read-next-story prev " style="background-image: url(/jasper/assets/images/cover4.jpg)" href="/jasper/my-first-post">
            <section class="post">
                <h2>My First Post</h2>
                <p>Hello world, this is my first post. The Jekyll theme is the Jasper Theme. $$\frac{x^2}{\sqrt{y+1}}$$...</p>
            </section>
        </a>
    
    <!-- [[! /prev_post ]] -->
</aside>

<!-- /post -->


        <footer class="site-footer clearfix">
          <section class="copyright"><a href="/jasper/">Firefly In The Darkness, Flittering About</a> &copy; 2016</section>
          <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> using <a href="https://github.com/biomadeira/jasper">Jasper</a></section>
        </footer>
    </div>
    <!-- [[! Ghost outputs important scripts and data with this tag ]] -->
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <!-- [[! The main JavaScript file for Casper ]] -->
    <script type="text/javascript" src="/jasper/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/jasper/assets/js/index.js"></script>

    <!-- Add Google Analytics  -->
        <!-- Google Analytics Tracking code -->
     <script>
	    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	    ga('create', 'UA-69281367-1', 'auto');
	    ga('send', 'pageview');

     </script>   
</body>
</html>
